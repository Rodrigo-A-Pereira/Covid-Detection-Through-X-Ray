{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "name": "TransferLearning.ipynb",
            "provenance": [],
            "collapsed_sections": []
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HSX1E74hY-dY"
            },
            "source": [
                "#Inicializations"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "KxVkccNbmVGo",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 125
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1592142618836,
                    "user_tz": -60,
                    "elapsed": 27820
                },
                "outputId": "6b848e8b-55ec-4556-8fb3-58f1d850708c"
            },
            "source": [
                "BASEDIR = '.'\n",
                "\n",
                "TRAINDATAPATH = 'aug_dataset_covid/train'\n",
                "\n",
                "TESTDATAPATH = 'aug_dataset_covid/test'\n",
                "\n",
                "\n",
                "VALIDDATAPATH = 'aug_dataset_covid/valid'\n",
                "\n",
                "NEWTESTDATAPATH = 'dataset_covid/testNew'"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4ovlfLNXnt5E"
            },
            "source": [
                "Make necessary imports"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "tclJoEw-lLac",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1592142620674,
                    "user_tz": -60,
                    "elapsed": 29642
                },
                "outputId": "5a7dc5e6-925b-4080-d8b0-e3bf7f03b92c"
            },
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow.keras import models\n",
                "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,\\\n",
                "                                    MaxPooling2D,BatchNormalization,\\\n",
                "                                    MaxPooling2D,AveragePooling2D,\\\n",
                "                                    GlobalMaxPooling2D,GlobalAveragePooling2D,\\\n",
                "                                    Concatenate,Input,ZeroPadding2D,Reshape\n",
                "                                    \n",
                "from keras.layers.merge import concatenate\n",
                "import numpy as np\n",
                "import time\n",
                "import os\n",
                "\n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "from keras.preprocessing.image import img_to_array, load_img\n",
                "from tensorflow.keras.callbacks import TensorBoard\n",
                "\n",
                "from tensorflow.keras.applications.resnet import ResNet50\n",
                "from tensorflow.keras.preprocessing import image\n",
                "from tensorflow.keras.models import Model\n",
                "\n",
                "from tensorflow.keras.applications.resnet import preprocess_input\n",
                "\n",
                "\n",
                "import matplotlib.pyplot as plt"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "PX98GhZ26_WI"
            },
            "source": [
                "#Corre\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "\n",
                "def plot_acc_loss(result, epochs):\n",
                "    acc = result.history['accuracy']\n",
                "    loss = result.history['loss']\n",
                "    val_acc = result.history['val_accuracy']\n",
                "    val_loss = result.history['val_loss']\n",
                "    plt.figure(figsize=(15, 5))\n",
                "    plt.subplot(121)\n",
                "    plt.plot(range(1,epochs), acc[1:], label='Train_acc')\n",
                "    plt.plot(range(1,epochs), val_acc[1:], label='Test_acc')\n",
                "    plt.title('Accuracy over ' + str(epochs) + ' Epochs', size=15)\n",
                "    plt.legend()\n",
                "    plt.grid(True)\n",
                "    plt.subplot(122)\n",
                "    plt.plot(range(1,epochs), loss[1:], label='Train_loss')\n",
                "    plt.plot(range(1,epochs), val_loss[1:], label='Test_loss')\n",
                "    plt.title('Loss over ' + str(epochs) + ' Epochs', size=15)\n",
                "    plt.legend()\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_RR71NkiZNyl"
            },
            "source": [
                "#Dataset\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "naSEPcPHZQTl"
            },
            "source": [
                "**Load for the first time the dataset**\n",
                "\n",
                "We first begin by defining the preprocesses that our data is going to suffer, in this case it suffers the recomended preprocess by RESNET as well as the possibility of beeing introduced some small alterations on the original image so our model generalzes better during training (small rotations, shears, zooming and mirroring)\n",
                "\n",
                "\n",
                "We make use of the flow_from_directory method available to the ImageDataGenerator object we created. This function will look at the given directory and separate the images in 2 classes based on our subdirectories.\n",
                "\n",
                "The images are given on 224 by 224, since its the recomended size to feed to a ResNet model.\n",
                "\n",
                "Since the generator is an iterator, the batch size decides how many elements are to be \"returned\" on each next() call.\n",
                "\n",
                "We also shuffle the data to avoid situations where the model receives to much of the same consecutive class, and ends up not generalizing but instead memorizing.\n",
                "\n",
                "\n",
                "**Note:** the validation is composed of 20% of the original training dataset"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "y4ooEeO6oxT5",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 70
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1586704176347,
                    "user_tz": -60,
                    "elapsed": 9303
                },
                "outputId": "7ba6679e-809b-4572-8156-8a743e99021d"
            },
            "source": [
                "\n",
                "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,rotation_range=30,shear_range=0.25,zoom_range=0.1, horizontal_flip = True) \n",
                "train_generator=train_datagen.flow_from_directory(TRAINDATAPATH, \n",
                "                                                 target_size=(224,224),\n",
                "                                                 batch_size=1,\n",
                "                                                 class_mode='binary',\n",
                "                                                 shuffle=True)\n",
                "\n",
                "print(type(train_generator))\n",
                "\n",
                "\n",
                "valid_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,rotation_range=30,shear_range=0.25,zoom_range=0.1, horizontal_flip = True) \n",
                "valid_generator=valid_datagen.flow_from_directory(VALIDDATAPATH, \n",
                "                                                 target_size=(224,224),\n",
                "                                                 batch_size=1,\n",
                "                                                 class_mode='binary',\n",
                "                                                 shuffle=True)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Td6I-SEwZbEv"
            },
            "source": [
                "See what numbers have been atributed to each class"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "QJUsgc0UZo87"
            },
            "source": [
                "print(train_generator.class_indices)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "n_IDM-akZw-U"
            },
            "source": [
                "Visualize the data\n",
                "\n",
                "Note that the images have already been processed by the resnet preprocessor, thus the color pallet and the image slight alterations"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "olK9yUVerNLk"
            },
            "source": [
                "x, y= next(train_generator)\n",
                "image = x_batch[0]\n",
                "plt.imshow(image,)\n",
                "plt.show()\n",
                "\n",
                "eval_generator.reset()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "bN6qONbTnz6b"
            },
            "source": [
                "#ResNet50\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8KLYHivpnvB-"
            },
            "source": [
                "Import **ResNet50Model**![alt text](https://miro.medium.com/max/2000/1*zbDxCB-0QDAc4oUGVtg3xw.png) "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7ShPolQpcGyO"
            },
            "source": [
                "Remove output layer from the base model, and get the weights of this networks trained for imagenet"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "feWVuB6fnnd4"
            },
            "source": [
                "base_model=ResNet50(include_top=False, weights='imagenet')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "GvN0CCXJd2-v"
            },
            "source": [
                "Add our own layers to the end of the model "
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "pYz3aiXVpJYM"
            },
            "source": [
                "x=base_model.output\n",
                "x=GlobalAveragePooling2D()(x)\n",
                "x=Dense(512,activation='relu')(x) \n",
                "x=Dropout(0.5)(x) \n",
                "preds=Dense(1,activation='sigmoid')(x)\n",
                "\n",
                "model=Model(inputs=base_model.input,outputs=preds)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XNCAaAZEd5HR"
            },
            "source": [
                "Freeze the weights on all layers except ours"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "xCbZEXuPpurF"
            },
            "source": [
                "for layer in model.layers[:-4]:\n",
                "    layer.trainable=False\n",
                "for layer in model.layers[-4:]:\n",
                "    layer.trainable=True\n",
                "    "
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Ae2PAhzUrFCe"
            },
            "source": [
                "Compile the model using the Adam Optimizer, and the binary loss entropy function since our problem is a binary classification one.\n",
                "\n",
                "The steps sizes are calculated given the total number of images and the batch sizes of the correspondent generators\n",
                "\n",
                "We also chose 20 epochs for this first test"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "dJ_LzsHT-ux6"
            },
            "source": [
                "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
                "\n",
                "step_size_train=train_generator.n//train_generator.batch_size\n",
                "step_size_valid=valid_generator.n//valid_generator.batch_size\n",
                "model.fit_generator(train_generator,\n",
                "                    steps_per_epoch =step_size_train,\n",
                "                    validation_data = valid_generator,\n",
                "                    validation_steps = step_size_valid,\n",
                "                    epochs= 20,\n",
                "                    )\n",
                " "
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dNZoSTmisaGk"
            },
            "source": [
                "Now that we verified that some results are obtained from the last train, we proceed to find the best hyper parameters:\n",
                "\n",
                "The followed parameters where testes for the following values:\n",
                "\n",
                "Number of dense layers : 1, 2, 3, 4\n",
                "\n",
                "Number of nodes per dense layer: 16, 32, 64, 128, 256\n",
                "\n",
                "Value of dropout: 0, 0.2, 0.5\n",
                "\n",
                "Batch Size: 1, 2, 3\n",
                "\n",
                "Number of epochs trained: 10. 20. 30\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "ZoXvXEXwqA7-"
            },
            "source": [
                "from tensorflow.keras.applications.resnet import preprocess_input\n",
                "import os\n",
                "from tensorflow.keras import optimizers\n",
                "\n",
                "NUMBER_DENSE = [2,3,4]\n",
                "DENSE_NODES = [16,32,64]\n",
                "DROPOUT = [0]\n",
                "BATCH_SIZE = [2]\n",
                "EPOCHS = [20]\n",
                "\n",
                "\n",
                "for denseNumber in NUMBER_DENSE:\n",
                "  for denseNodes in DENSE_NODES:\n",
                "    for dropoutNumber in DROPOUT:\n",
                "      for batchSize in BATCH_SIZE:\n",
                "        for epochNumber in EPOCHS:\n",
                "        \n",
                "\n",
                "          LOGNAME = \"RESNET50_RUN2.0_DenseLayers-{0}_DenseNodes-{1}_Dropout-{2}_BatchSize-{3}_EpochNumber-{4}\".format(denseNumber,denseNodes,dropoutNumber,batchSize,epochNumber)\n",
                "          log_dir = os.path.join(BASEDIR,\"logs\",LOGNAME,\"fit\",)\n",
                "          tensorboard = TensorBoard(log_dir = log_dir)\n",
                "          print(LOGNAME)\n",
                "\n",
                "          train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,rotation_range=30,shear_range=0.25,zoom_range=0.1, horizontal_flip = True) \n",
                "          train_generator=train_datagen.flow_from_directory(TRAINDATAPATH, \n",
                "                                                          target_size=(224,224),\n",
                "                                                          batch_size=batchSize,\n",
                "                                                          class_mode='binary',\n",
                "                                                          shuffle=True)\n",
                "\n",
                "\n",
                "          valid_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,rotation_range=30,shear_range=0.25,zoom_range=0.1, horizontal_flip = True) \n",
                "          valid_generator=valid_datagen.flow_from_directory(VALIDDATAPATH, \n",
                "                                                          target_size=(224,224),\n",
                "                                                          batch_size=batchSize,\n",
                "                                                          class_mode='binary',\n",
                "                                                          shuffle=True)\n",
                "\n",
                "          nOfNew = -2 \n",
                "          base_model=ResNet50(include_top=False, weights='imagenet')\n",
                "          x=base_model.output\n",
                "          x=GlobalAveragePooling2D()(x)\n",
                "\n",
                "          for i in range(denseNumber-1):\n",
                "            x=Dense(denseNodes,activation='relu')(x) \n",
                "            x=Dropout(dropoutNumber)(x) \n",
                "            nOfNew-=2\n",
                "\n",
                "          preds=Dense(1,activation='sigmoid')(x)\n",
                "\n",
                "          model=Model(inputs=base_model.input,outputs=preds)\n",
                "\n",
                "\n",
                "          for layer in model.layers[:nOfNew]:\n",
                "              layer.trainable=False\n",
                "          for layer in model.layers[nOfNew:]:\n",
                "              layer.trainable=True\n",
                "              \n",
                "\n",
                "          model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=0.0005),metrics=['accuracy'])\n",
                "\n",
                "          step_size_train=train_generator.n//train_generator.batch_size\n",
                "          step_size_valid=valid_generator.n//valid_generator.batch_size\n",
                "          model.fit_generator(train_generator,\n",
                "                              steps_per_epoch =step_size_train,\n",
                "                              validation_data = valid_generator,\n",
                "                              validation_steps = step_size_valid,\n",
                "                              epochs= epochNumber,\n",
                "                              callbacks=[tensorboard]\n",
                "                              )\n",
                "\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "EwlrTkvftCiY"
            },
            "source": [
                "After this intensive search the best results where found looking to the graphs given by the TensorBoard platform (and saved in the logs directory)\n",
                "\n",
                "\n",
                "**Train accuracy:**\n",
                "\n",
                "![alt text](https://cdn.discordapp.com/attachments/498949663513116673/698239341406978048/unknown.png)\n",
                "\n",
                "**Train loss:**\n",
                "\n",
                "![alt text](https://cdn.discordapp.com/attachments/498949663513116673/698239373052739615/unknown.png)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "**Validation accuracy:**\n",
                "\n",
                "![alt text](https://cdn.discordapp.com/attachments/498949663513116673/698239220145455115/unknown.png)\n",
                "\n",
                "\n",
                "**Validation loss:**\n",
                "\n",
                "![alt text](https://cdn.discordapp.com/attachments/498949663513116673/698239373052739615/unknown.png)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "nXsFY5pWv3J5"
            },
            "source": [
                "We then verified that the best results where given for the following parameters:\n",
                "\n",
                "Number of dense layers: 2 and 3 (whe ended up chosing 2 for training time reasons since there was no noticable difference in preformace with2 or 3 layers)\n",
                "\n",
                "Number of nodes per dense layer: 32 or 64 (similarly to the number of layers the diference between this numbers werent noticable we opted to chose based on the one that reduced the training time - 32)\n",
                "\n",
                "The batch size number variation didnt seem to affect at all: so we ended up chosing 2\n",
                "\n",
                "The dropout inclusion gave worst results than the ones without so we ended up not using one:\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XatLYw2vx6V9"
            },
            "source": [
                "Bellow is the final model for training:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "g7uXZ4GScC41",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "executionInfo": {
                    "status": "error",
                    "timestamp": 1592142847460,
                    "user_tz": -60,
                    "elapsed": 75471
                },
                "outputId": "e3d4bb13-f4ad-4af5-b307-a160900ae606"
            },
            "source": [
                "from tensorflow.keras.applications.resnet import preprocess_input\n",
                "import os\n",
                "from tensorflow.keras import optimizers\n",
                "\n",
                "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,rotation_range=30,shear_range=0.25,zoom_range=0.1, horizontal_flip = True) \n",
                "train_generator=train_datagen.flow_from_directory(TRAINDATAPATH, \n",
                "                                                target_size=(224,224),\n",
                "                                                batch_size=1,\n",
                "                                                class_mode='binary',\n",
                "                                                shuffle=True)\n",
                "\n",
                "\n",
                "valid_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,rotation_range=30,shear_range=0.25,zoom_range=0.1, horizontal_flip = True) \n",
                "valid_generator=valid_datagen.flow_from_directory(VALIDDATAPATH, \n",
                "                                                target_size=(224,224),\n",
                "                                                batch_size=1,\n",
                "                                                class_mode='binary',\n",
                "                                                shuffle=True)\n",
                "\n",
                " \n",
                "base_model=ResNet50(include_top=False, weights='imagenet')\n",
                "x=base_model.output\n",
                "x=GlobalAveragePooling2D()(x)\n",
                "\n",
                "\n",
                "x=Dense(32,activation='relu')(x) \n",
                "x=Dense(32,activation='relu')(x) \n",
                "\n",
                "\n",
                "preds=Dense(1,activation='sigmoid')(x)\n",
                "\n",
                "model=Model(inputs=base_model.input,outputs=preds)\n",
                "\n",
                "\n",
                "for layer in model.layers[:-4]:\n",
                "    layer.trainable=False\n",
                "for layer in model.layers[-4:]:\n",
                "    layer.trainable=True\n",
                "    \n",
                "\n",
                "model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=0.0005),metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.TruePositives(),  tf.keras.metrics.TrueNegatives(),  tf.keras.metrics.FalsePositives(),  tf.keras.metrics.FalseNegatives()])\n",
                "\n",
                "model.summary()\n",
                "step_size_train=train_generator.n//train_generator.batch_size\n",
                "step_size_valid=valid_generator.n//valid_generator.batch_size\n",
                "history=model.fit_generator(train_generator,\n",
                "                    steps_per_epoch =step_size_train,\n",
                "                    validation_data = valid_generator,\n",
                "                    validation_steps = step_size_valid,\n",
                "                    epochs= 20,\n",
                "                    )"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "8XDVg_0AJDJM",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 337
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1586708543768,
                    "user_tz": -60,
                    "elapsed": 2310
                },
                "outputId": "5fd6e979-0c21-42a3-af21-5ea460b31057"
            },
            "source": [
                "#Corre\n",
                "plot_acc_loss(history, 20)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OT1ljx9Lx9Vh"
            },
            "source": [
                "We proceed to test the model against the test set (20% of the original dataset)"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "DJe8UP_wdD33",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1586730103752,
                    "user_tz": -60,
                    "elapsed": 346760
                },
                "outputId": "60b37d7d-6ebb-4ac7-9ef0-314e76fb7b58"
            },
            "source": [
                "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
                "\n",
                "eval_generator = test_datagen.flow_from_directory(\n",
                "        TESTDATAPATH,\n",
                "        target_size=(224,224),\n",
                "        batch_size=1,\n",
                "        shuffle=False,\n",
                "        seed=42,\n",
                "        class_mode=\"binary\")\n",
                "\n",
                "\n",
                "\n",
                "eval_generator.reset()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "igfsQTDSdjdG",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 176
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1586730138401,
                    "user_tz": -60,
                    "elapsed": 381401
                },
                "outputId": "f82a3d7b-294a-42ed-edde-4c2461703735"
            },
            "source": [
                "x = model.evaluate_generator(eval_generator,\n",
                "                           steps = np.ceil(len(eval_generator) / 1),\n",
                "                           use_multiprocessing = False,\n",
                "                           verbose = 1,\n",
                "                           workers=1\n",
                "                           )\n",
                "\n",
                "\n",
                "print('Test loss:' , x[0])\n",
                "print('Test accuracy:',x[1])\n",
                "print('Test precision:' , x[2])\n",
                "print('Test Recall:',x[3])\n",
                "print('Test F1 Score: ',2*(x[2]*x[3])/(x[2]+x[3]))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "FSL_cyyvyIan"
            },
            "source": [
                "We finalize by visualizing the predictions of th model"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "sBZmDdpohQQ_"
            },
            "source": [
                "eval_generator.reset()  \n",
                "pred = model.predict_generator(eval_generator,steps = np.ceil(len(eval_generator) / 1),verbose=1)\n",
                "print(\"Predictions finished\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "1IVzAHUMcc3a",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 301
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1586731385553,
                    "user_tz": -60,
                    "elapsed": 1059
                },
                "outputId": "93b60d70-00a2-4bc8-feb6-d8ef25935fab"
            },
            "source": [
                "import seaborn as sn\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "trueP= x[4]\n",
                "trueN = x[5]\n",
                "falseP = x[6]\n",
                "falseN = x[7]\n",
                "array = [[trueP,falseP],\n",
                "         [falseN,trueN]]\n",
                "\n",
                "array = [[trueP,falseP], [falseN,trueN]]\n",
                "df_cm = pd.DataFrame(array, [\"non-covid19\",\"covid19\"], [\"non-covid19\",\"covid19\"])\n",
                "sn.set(font_scale=1.4)\n",
                "\n",
                "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},cmap=\"Blues\") "
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "sQ4OfWsOhgrY",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000,
                    "output_embedded_package_id": "13z2JPIu8HzlnWc1Asoc42PmDLdNi9FK4"
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1586704597420,
                    "user_tz": -60,
                    "elapsed": 430331
                },
                "outputId": "17d4b479-b9a8-4b7d-c717-91eb0402a2b4"
            },
            "source": [
                "import cv2\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.image as mpimg\n",
                "\n",
                "for index, probability in enumerate(pred):\n",
                "    image_path = TESTDATAPATH + \"/\" +eval_generator.filenames[index]\n",
                "    image = mpimg.imread(image_path)\n",
                "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "    pixels = np.array(image)\n",
                "    plt.imshow(pixels)\n",
                "    \n",
                "    print(eval_generator.filenames[index])\n",
                "    if probability > 0.5:\n",
                "        plt.title(\"%.2f\" % (probability[0]*100) + \"% Non-COVID19\")\n",
                "    else:\n",
                "        plt.title(\"%.2f\" % ((1-probability[0])*100) + \"% COVID19\")\n",
                "    plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        }
    ]
}